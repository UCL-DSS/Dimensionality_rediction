{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e158ad51",
   "metadata": {},
   "source": [
    "# DS - Python with Data Science\n",
    "\n",
    "## Dimensionality reduction\n",
    "\n",
    "### Solutions\n",
    "\n",
    "### Debug code\n",
    "\n",
    "### 1) Run a Random forest classification algorithm\n",
    "\n",
    "Run a random forest classification algorithm on the data and time how long it takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1403d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "NBA = pd.read_csv(\"Data/NBA_cleaned.csv\")\n",
    "#drop the necessary columns\n",
    "NBA.drop(columns = ['Component 1', 'Component 2',\n",
    "                   'Dimension 1', 'Dimension 2'],\n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82734c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the columns\n",
    "NBA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an X and y dataset based on the variables\n",
    "#you want to use\n",
    "X = NBA.drop(columns= [??, ??])\n",
    "y = NBA.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b044782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into a training and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(??, ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b92fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the random forest classifier\n",
    "#with 100 estimators\n",
    "#max depth of 10\n",
    "clf = RandomForestClassifier(n_estimators =??, \n",
    "                             oob_score=??, \n",
    "                             max_depth =??,\n",
    "                            random_state = ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6676900",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#fit it to the training data\n",
    "#and chekc the time\n",
    "clf.fit(??, ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe76f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the predictions\n",
    "test_pred_random_forest = clf.predict(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confusion matrix\n",
    "confusion_matrix2 = metrics.confusion_matrix(y_test,\n",
    "                                             test_pred_random_forest)\n",
    "\n",
    "#turn this into a dataframe\n",
    "matrix2_df = pd.DataFrame(confusion_matrix2)\n",
    "\n",
    "# set axis to add title and axis labels later\n",
    "ax = plt.axes()\n",
    "sns.set(font_scale=1.3) # for label size\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "plot = sns.heatmap(matrix2_df, \n",
    "                   annot=True, \n",
    "                   fmt='g', \n",
    "                   ax=ax, \n",
    "                   cmap = \"magma\") \n",
    "#fmt so that numbers aren't scientific\n",
    "\n",
    "#axis labels and title\n",
    "ax.set_title('Confusion Matrix - Random Forest', fontsize = 17)\n",
    "ax.set_xlabel('Predicted Label', fontsize = 15)\n",
    "ax.set_ylabel('True Label'     , fontsize = 15)\n",
    "\n",
    "# change tick labels from 0-4 to 1-5\n",
    "labels = (\"C\", \"PF\", \"PG\", \"SF\", \"SG\")\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plot.get_figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the metrics classification report\n",
    "print (metrics.classification_report(y_test, test_pred_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b878522",
   "metadata": {},
   "source": [
    "### 2) Run a Random forest classification algorithm\n",
    "\n",
    "But this time after the data has been passed through a PCA algorithm and compare the difference in time and accuracy in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the standard scaler\n",
    "sc = ??\n",
    "\n",
    "#extract the columns you want to rescale\n",
    "cols = ['FG', 'FGA', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB',\n",
    "       'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', '3P%',\n",
    "       '2P%', 'FT%', 'FG%', 'eFG%']\n",
    "\n",
    "#fit the scalers on the data\n",
    "scalers = [sc.fit(NBA[??].values.reshape(-1,1)) for c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the X_train and X_test dataset\n",
    "tr_scaled = X_train[cols].copy()\n",
    "tst_scaled = X_test[cols].copy()\n",
    "\n",
    "#transform all the columns\n",
    "#users the scalers we created\n",
    "for i in range(0, len(cols)):\n",
    "    tr_scaled[cols[i]] = (scalers[i]\n",
    "                          .transform(\n",
    "                              X_train[cols[i]]\n",
    "                              .values\n",
    "                              .reshape(-1,1)))\n",
    "    tst_scaled[cols[i]] = (scalers[i]\n",
    "                          .transform(\n",
    "                              X_test[cols[i]]\n",
    "                              .values\n",
    "                              .reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the pca with 15 components\n",
    "pca = PCA(??, ??)\n",
    "#fit it to the scaled dataset you crete\n",
    "pca.fit(??)\n",
    "\n",
    "#extract the explained variance\n",
    "explained_variance = pca.??\n",
    "singular_values = pca.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241eb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an x for each component\n",
    "x = np.arange(1,len(explained_variance)+1)\n",
    "\n",
    "#plot the results\n",
    "plt.plot(x, ??)\n",
    "\n",
    "#add a y label\n",
    "plt.ylabel('Share of Variance Explained')\n",
    "plt.title(\"PCA explained variance plot\")\n",
    "plt.xlabel(\"Components\")\n",
    "\n",
    "#show the resuling plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the components\n",
    "#to print the explained variance\n",
    "for i in range(0, 15):\n",
    "    print(f\"Component {i:>2} accounts for {explained_variance[i]*100:>2.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ca3ad",
   "metadata": {},
   "source": [
    "Extract the number of components based on the 1% cutoff (if you wanted you could experiment with different cut off methods and how that may affect the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the components to ??\n",
    "pca = PCA(n_components=??, whiten=True) \n",
    "#fit the model to our data and extract the results\n",
    "pca.fit(tr_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract a train and test pca dataset\n",
    "train_pca = pca.transform(??)\n",
    "test_pca = pca.transform(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26896bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#fit it to the training data\n",
    "#make note of the time\n",
    "clf.fit(??, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the predictions\n",
    "test_pred_random_forest = clf.predict(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc84259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the confusion matrix\n",
    "confusion_matrix2 = metrics.confusion_matrix(y_test, test_pred_random_forest)\n",
    "\n",
    "\n",
    "matrix2_df = pd.DataFrame(confusion_matrix2)\n",
    "\n",
    "# set axis to add title and axis labels later\n",
    "ax = plt.axes()\n",
    "sns.set(font_scale=1.3) # for label size\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "plot = sns.heatmap(matrix2_df, annot=True, fmt='g', ax=ax, cmap = \"magma\") #fmt so that numbers aren't scientific\n",
    "\n",
    "#axis labels and title\n",
    "ax.set_title('Confusion Matrix - Random Forest', fontsize = 17)\n",
    "ax.set_xlabel('Predicted Label', fontsize = 15)\n",
    "ax.set_ylabel('True Label'     , fontsize = 15)\n",
    "\n",
    "# change tick labels from 0-4 to 1-5\n",
    "labels = (\"C\", \"PF\", \"PG\", \"SF\", \"SG\")\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plot.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the metrics\n",
    "print (metrics.classification_report(y_test, test_pred_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abedc82",
   "metadata": {},
   "source": [
    "What has happened to the tim it takes for the model to fit and the resulting performance of the model? Do you think this is a good trade-off? How do you think this will work with a much, much, much larger dataset (both in terms of rows and columns?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccadc8",
   "metadata": {},
   "source": [
    "### 3) Fit a t-SNE model with two dimensions on the whole data after using PCA\n",
    "\n",
    "This time, run a PCA model first to cut down the data to several principle components then run a t-SNE algorithm. What do you notice about the outcome compared to the workshop results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396cda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the columns to scale\n",
    "cols = ['FG', 'FGA', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB',\n",
    "       'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', '3P%',\n",
    "       '2P%', 'FT%', 'FG%', 'eFG%']\n",
    "\n",
    "#extract the total X variables\n",
    "X_scaled = X.copy()\n",
    "\n",
    "#scale the results using fit_transform\n",
    "for col in cols:\n",
    "    X_scaled[col] = sc.fit_transform(X_scaled[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26460eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run The PCA algorithm with 15 components\n",
    "pca = PCA(??, ??) \n",
    "\n",
    "#fit it to our data\n",
    "pca.fit(??)\n",
    "\n",
    "#extract the explained variance\n",
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an x for each component\n",
    "x = np.arange(1,len(explained_variance)+1)\n",
    "\n",
    "#plot the results\n",
    "??\n",
    "\n",
    "#add a y label\n",
    "plt.ylabel('Share of Variance Explained')\n",
    "plt.title(\"PCA explained variance plot\")\n",
    "plt.xlabel(\"Components\")\n",
    "\n",
    "#show the resuling plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f67efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the components\n",
    "#to print the explained variance\n",
    "for i in range(0, 15):\n",
    "    print(f\"Component {i:>2} accounts for {explained_variance[i]*100:>2.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e87aa1",
   "metadata": {},
   "source": [
    "Extract the components that account for 80% of the variance in the variables (what happens if you choose a different selection criteria?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the components to ??\n",
    "pca = PCA(n_components=??, whiten=True) \n",
    "#fit the model to our data and extract the results\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe from the dataset\n",
    "df = pd.DataFrame(data = ??,\n",
    "                 columns = [??])\n",
    "\n",
    "#merge this with the NBA data\n",
    "NBA = pd.merge(NBA,\n",
    "              df,\n",
    "              left_index=True,\n",
    "              right_index=True,\n",
    "              how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1418251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the hyperparmateres for the model with\n",
    "#the same as in the workshop\n",
    "#try playing around with them\n",
    "#how does that affect the outcome?\n",
    "keep_dims = ??\n",
    "lrn_rate = ??\n",
    "prp = ??\n",
    "\n",
    "#create the model\n",
    "tsne = TSNE(n_components = keep_dims, \n",
    "            perplexity = prp, \n",
    "            random_state = 42,\n",
    "            n_iter = 5000,\n",
    "            n_jobs = -1)\n",
    "\n",
    "#extract the 7 components from the NBA dataset\n",
    "tsne_df = NBA.loc[:,'Component 1':f\"Component 7\"].copy()\n",
    "\n",
    "#fit the model\n",
    "X_embedded = tsne.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe from the dataset\n",
    "df = pd.DataFrame(data = ??,\n",
    "                 columns = [\"Dimension 1\", \n",
    "                            \"Dimension 2\"])\n",
    "\n",
    "#mere the dataset back with the NBA data\n",
    "NBA = pd.merge(NBA,\n",
    "              df,\n",
    "              left_index = True,\n",
    "              right_index = True,\n",
    "              how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8272d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the result\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e9048",
   "metadata": {},
   "source": [
    "How is this different to just using the t-SNE algorithm? Is it clearer or less clearer? Why may this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8d79f",
   "metadata": {},
   "source": [
    "## Find your own dataset to explore this on your own"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSS",
   "language": "python",
   "name": "dss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
